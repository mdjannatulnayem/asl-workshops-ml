{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL ML Workshop - Review of Day 3\n",
    "- Introduction to scikit learn https://scikit-learn.org/\n",
    "- Prep data with pandas\n",
    "- Building logistic regression model using scikit learn\n",
    "- Predicting with logistic regression model\n",
    "- Scoring the model\n",
    "- Model evaluation (evaluation metrics)\n",
    "- Train - test split\n",
    "- Build the model with splitted dataset\n",
    "- ROC foundation (specificity & sensitivity)\n",
    "- Adjusting the threshold for logistic regression\n",
    "- The ROC curve\n",
    "- Area under curve\n",
    "- Comparision of two models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Jupyter notebook?\n",
    "Jupyter Notebook is an interactive computing environment that allows users to create and share documents that contain live code, equations, visualizations, and narrative text. It is an open-source web application that supports over 100 programming languages, including Python, R, and Julia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn\n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning that provides a range of tools for various tasks such as classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. It is built on top of NumPy, SciPy, and matplotlib, which are also popular scientific computing libraries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data with pandas\n",
    "\n",
    "Prepare the training data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df # our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature column\n",
    "\n",
    "df['Male'] = df['Sex'] == 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create numpy array for training\n",
    "\n",
    "X = df[['Pclass','Male','Age','Siblings/Spouses','Parents/Children','Fare']].values\n",
    "\n",
    "y = df['Survived'].values\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building logistic regression model\n",
    "\n",
    "Logistic regression is a popular statistical model used for binary classification problems, where the goal is to predict a binary outcome (e.g., yes/no, 0/1) based on a set of input features. It is a type of linear model that uses a logistic function to model the probability of the binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LgR\n",
    "\n",
    "model = LgR() # create an instance of LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with our dataset\n",
    "Training the model with titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y) # passing the feature array X and the target y\n",
    "\n",
    "# now the model will find the best parameters for our classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_,model.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with logistic regression model\n",
    "Making prediction with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for a random person\n",
    "print(model.predict([[3,True,22,0,1,7.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for first 5 entry in the dataset\n",
    "print(model.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual survival data \\\n",
    "# for first 5 entry in the dataset\n",
    "print(y[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the model\n",
    "Scoring the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y == y_pred).sum() # number of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "print((y == y_pred).sum()/y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(X,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Evaluation metrics\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import precision_score as prec\n",
    "from sklearn.metrics import recall_score as rec\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix as cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"Accuracy :\",acc(y,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A 2 x 2 matrix\n",
    "- 0th row actually positive\n",
    "- 1st row actually negative\n",
    "- 0th column predicted positive\n",
    "- 0th column predicted negative\n",
    "- (0,0) => True positives, TP\n",
    "- (0,1) => False positives, FP\n",
    "- (1,0) => False negatives, FN\n",
    "- (1,1) => True negatives, TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\\n\",cfm(y,y_pred))\n",
    "# this will produce the matrix in reverse order \\\n",
    "# this is because 0 correspond to negative and 1 correspond to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precission = TP / (TP + FP)\n",
    "print(\"Precision :\",prec(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = TP / (TP + FN)\n",
    "print(\"Recall :\",rec(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 score :\",f1(y,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - test split\n",
    "Use separate data for traing and testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from sklearn.model_selection import train_test_split as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train,X_test,y_train,y_test = split(X,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full dataset :\",X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset: \",X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset: \",X_test.shape,y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild the model with splitted dataset\n",
    "This will provide more accurate evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = LgR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.fit(X_train,y_train)\n",
    "y_pred = model_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy score\n",
    "print(\"Accuracy :\",acc(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision score\n",
    "print(\"Precision :\",prec(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall score\n",
    "print(\"Recall score :\",rec(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score\n",
    "print(\"F1 score :\",f1(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity & Sensitivity\n",
    "Foundation of ROC (Reciever Operating Characteristic) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = rec # sensitivity is another term for recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificity = TN / (TN + FN) \\\n",
    "# that is also the recall of the negative class\n",
    "\n",
    "def specificity(y,y_pred):\n",
    "    p,r,f,s = prfs(y,y_pred)\n",
    "    return r[0] # r[0] is the recall of the negative class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sensitivity(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(specificity(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the threshold\n",
    "Tweeking the default threshold values can be beneficial to boost precision or recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba(X_test)\n",
    "\n",
    "The result is a numpy array with 2 values for each datapoint i.e., the first value is the probability that the datapoint is in the 0 class and the second value is the probality of the datapoint being in the 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_new.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.75\n",
    "# set the new threshold \\\n",
    "# default is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_new.predict_proba(X_test)[:,1] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y_test == y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision :\",prec(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve\n",
    "Graphical evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve as roc\n",
    "from sklearn.metrics import roc_auc_score as auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain specificity and sensitivity for every possible threshold\n",
    "fpr,tpr,thresholds = roc(y_test,model_new.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('sensitivity')\n",
    "plt.xlabel('1 - specificity')\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area under curve\n",
    "Numeric measure of the area covered by the curve. Maximum is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model_new.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc score\n",
    "print(auc(y_test,y_pred_proba[:,1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparision of 2 different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LgR()\n",
    "# creating a new model with fewer features\n",
    "model2.fit(X_train[:,0:2],y_train)\n",
    "y_pred_proba2 = model2.predict_proba(X_test[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc(y_test,y_pred_proba2[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr2,tpr2,thresholds2 = roc(y_test,y_pred_proba2[:,1])\n",
    "plt.ylabel('sensitivity')\n",
    "plt.xlabel('1 - specificity')\n",
    "plt.plot(fpr,tpr,color = 'green')\n",
    "plt.plot(fpr2,tpr2,color = 'red')\n",
    "plt.plot([0,1],[0,1],linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation\n",
    "Instead of doing a single train/test split, we'll split our data into a training set and test set multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from sklearn.model_selection import KFold as kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're doing 3 splits\n",
    "kfold = kf(n_splits = 3,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a smaller dataset\n",
    "X_small = df[['Age','Fare']].values[:6]\n",
    "y_small = df['Survived'].values[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train,test in kfold.split(X_small):\n",
    "    print(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now take the full dataset\n",
    "splits = list(kfold.split(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_split = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test datapoints :\",first_split[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices,test_indices = first_split\n",
    "print(\"Test set indices :\\n\",test_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 3 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for train_index,test_index in splits:\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    model = LgR()\n",
    "    model.fit(X_train,y_train)\n",
    "    scores.append(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model finalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LgR()\n",
    "final_model.fit(X,y)\n",
    "print(final_model.score(X,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
