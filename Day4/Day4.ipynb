{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Decision Trees\n",
    "Decision Trees unlike Logistic Regression are an example of a nonparametric machine learning algorithm. Decision Trees won’t be defined by a list of parameters as we’ll see in the upcoming lessons. The reason many people love decision trees is because they are very easy to interpret. It is basically a flow chart of questions that you answer about a datapoint until you get to a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary import\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe\n",
    "df = pd.read_csv('../titanic.csv')\n",
    "# create new column\n",
    "df['Male'] = df['Sex'] == 'male'\n",
    "# convert to array\n",
    "features = ['Pclass','Male','Age','Siblings/Spouses','Parents/Children','Fare']\n",
    "X = df[features].values\n",
    "y = df['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "X_train, X_test, y_train, y_test = split(X, y, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n"
     ]
    }
   ],
   "source": [
    "# creating a decision tree model\n",
    "model = DT()\n",
    "model.fit(X_train,y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([[3, True, 22, 1, 0, 7.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - gini\n",
      "accuracy: 0.7643940836666031\n",
      "precision: 0.6927375779554301\n",
      "recall: 0.7061688760011047 \n",
      "\n",
      "\n",
      "Decision Tree - entropy\n",
      "accuracy: 0.758795150130134\n",
      "precision: 0.6907424299434834\n",
      "recall: 0.6775865633482734 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "    precision_score,recall_score,f1_score,\n",
    "    precision_recall_fscore_support,confusion_matrix)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for criterion in ['gini', 'entropy']:\n",
    "    print(\"Decision Tree - {}\".format(criterion))\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        dt = DT(criterion=criterion)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "    print(\"accuracy:\", np.mean(accuracy))\n",
    "    print(\"precision:\", np.mean(precision))\n",
    "    print(\"recall:\", np.mean(recall), '\\n')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Decision Tree\n",
    "We want to create a png image of our graph. We'll use scikit-learn's export_graphviz function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz as export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph objects are stored as .dot files\n",
    "dot_file = export(model,feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dt_titanic.png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the 'graphviz' executable\n",
    "import graphviz as visual\n",
    "graph = visual.Source(dot_file)\n",
    "file = 'dt_titanic'\n",
    "graph.render(file,format='png',cleanup=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Decision Trees\n",
    "Decision Trees are incredibly prone to overfitting. Since they can keep having additional nodes in the tree that split on features, the model can really dig deep into the specifics of the training set.We have a few options for how to limit the tree growth. Here are some commonly used pre-pruning techniques.\n",
    "- Max depth: Only grow the tree up to a certain depth, or height of the tree\n",
    "- Leaf size: Don’t split a node if the number of samples at that node is under a threshold\n",
    "- Number of leaf nodes: Limit the total number of leaf nodes allowed in the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=4, max_leaf_nodes=20, min_samples_leaf=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=4, max_leaf_nodes=20, min_samples_leaf=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=4, max_leaf_nodes=20, min_samples_leaf=10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruning our tree\n",
    "dt = DT(max_depth=4,min_samples_leaf=10,max_leaf_nodes=20)\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(model,features,name):\n",
    "    dot_file = export(model,feature_names=features)\n",
    "    graph = visual.Source(dot_file)\n",
    "    graph.render(name,format='png',cleanup=True)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw(dt,features,'dt_titanic_prepruned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "We’re not going to be able to intuit best values for the pre-pruning parameters. But scikit-learn has a grid search class built in that will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV as grd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV has four parameters that we’ll use:\n",
    "1. The model (in this case a DecisionTreeClassifier)\n",
    "2. Param grid: a dictionary of the parameters names and all the possible values\n",
    "3. What metric to use (default is accuracy)\n",
    "4. How many folds for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_leaf_nodes':[15,20,25,30],\n",
    "    'max_depth':[5,8,10,12],\n",
    "    'min_samples_leaf':[5,10,15]\n",
    "    \n",
    "}\n",
    "# creat a grid search object\n",
    "gs = grd(dt,param_grid,scoring='f1',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 12, 'max_leaf_nodes': 15, 'min_samples_leaf': 5}\n"
     ]
    }
   ],
   "source": [
    "gs.fit(X,y)\n",
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7679461659113965\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score:\",gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LgR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_logistic_regression_decision_tree():\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    dt_accuracy_scores = []\n",
    "    dt_precision_scores = []\n",
    "    dt_recall_scores = []\n",
    "    lr_accuracy_scores = []\n",
    "    lr_precision_scores = []\n",
    "    lr_recall_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        dt = DT()\n",
    "        dt.fit(X_train, y_train)\n",
    "        dt_accuracy_scores.append(dt.score(X_test, y_test))\n",
    "        dt_y_pred = dt.predict(X_test)\n",
    "        dt_precision_scores.append(precision_score(y_test, dt_y_pred))\n",
    "        dt_recall_scores.append(recall_score(y_test, dt_y_pred))\n",
    "        lr = LgR()\n",
    "        lr.fit(X_train, y_train)\n",
    "        lr_accuracy_scores.append(lr.score(X_test, y_test))\n",
    "        lr_y_pred = lr.predict(X_test)\n",
    "        lr_precision_scores.append(precision_score(y_test, lr_y_pred))\n",
    "        lr_recall_scores.append(recall_score(y_test, lr_y_pred))\n",
    "    print(\"Decision Tree\")\n",
    "    print(\"\\tAccuracy:\", np.mean(dt_accuracy_scores))\n",
    "    print(\"\\tPrecision:\", np.mean(dt_precision_scores))\n",
    "    print(\"\\tRecall:\", np.mean(dt_recall_scores))\n",
    "    print(\"Logistic Regression\")\n",
    "    print(\"\\tAccuracy:\", np.mean(lr_accuracy_scores))\n",
    "    print(\"\\tPrecision:\", np.mean(lr_precision_scores))\n",
    "    print(\"\\tRecall:\", np.mean(lr_recall_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "\tAccuracy: 0.7812988002285279\n",
      "\tPrecision: 0.7149516065626427\n",
      "\tRecall: 0.7275785783920206\n",
      "Logistic Regression\n",
      "\tAccuracy: 0.8015743033073065\n",
      "\tPrecision: 0.7649462243249993\n",
      "\tRecall: 0.7027226721625388\n"
     ]
    }
   ],
   "source": [
    "compare_logistic_regression_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
